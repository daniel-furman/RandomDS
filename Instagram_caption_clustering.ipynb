{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Caption Style Segmentation for Instagram Food Influencers\n",
    "\n",
    "---\n",
    "\n",
    "[https://dryanfurman.shinyapps.io/caption_clustering/](https://dryanfurman.shinyapps.io/caption_clustering/).\n",
    "\n",
    "This notebook explores style variability in Instagram captions created by better-for-you food influencers (from Sep and Aug). I attempt to focus on style rather than topic; therefore, I filtered out any posts that were not about food and any text that uniqely identified to the profile (for example, their name).\n",
    "\n",
    "I used KMeans clustering on BERT embeddings and sparse word presences (nmi = 0.9), revealing signficant style similarity between profiles. I found that it was easier to cluster captions by profile relative to, for example, clustering poems by type (i.e., in [Literary Pattern Recognition](https://www.journals.uchicago.edu/doi/pdf/10.1086/684353)). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "    \n",
    "![dataviz](Insta-caption-style-viz.png)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "source": [
    "import nltk\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "import csv, os, re\n",
    "import math\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "source": [
    "def read_texts(dir_folder):\n",
    "    \n",
    "    paths = glob(dir_folder)\n",
    "    txt_list = []\n",
    "    for path in paths:\n",
    "        txt = Path(path).read_text()\n",
    "        txt = txt.replace('\\n', '')\n",
    "        txt_list.append(txt)\n",
    "\n",
    "    return txt_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "source": [
    "rachel_mansfield=read_texts(\"../data/insta-food-influencers/rachelmansfield/*.txt\")\n",
    "#rachel_mansfield=sample(rachel_mansfield, 40)\n",
    "minimalist_baker=read_texts(\"../data/insta-food-influencers/minimalistbaker/*.txt\")\n",
    "minimalist_baker=sample(minimalist_baker,54)\n",
    "delicioushealthyvideos=read_texts(\"../data/insta-food-influencers/delicioushealthyvideos/*.txt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "source": [
    "print(len(rachel_mansfield), len(minimalist_baker), len(delicioushealthyvideos))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "54 54 54\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "source": [
    "def run_all(influencer1, influencer2, influencer3, feature_function):\n",
    "    X, Y, feature_list=feature_function(influencer1, influencer2, influencer3)\n",
    "    kmeans = KMeans(n_clusters=3, random_state=46).fit(X)\n",
    "    nmi=metrics.normalized_mutual_info_score(Y, kmeans.labels_)\n",
    "    print(\"%.3f NMI\" % nmi)\n",
    "\n",
    "    return X, Y, kmeans.labels_\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "source": [
    "# This function takes in a list of Instagram captions from two influencers, and returns:\n",
    "\n",
    "# X (sparse matrix, with captions as rows and features as columns)\n",
    "# Y (list of caption labels, with 1=minimalist_baker and 0=rachel_mansfield)\n",
    "\n",
    "def featurize_method_1(influencer1, influencer2, influencer3):\n",
    "\n",
    "    def featurize(caption, feature_vocab):\n",
    "                \n",
    "        feats={}\n",
    "\n",
    "        tokens=nltk.word_tokenize(caption.lower())\n",
    "        for token in tokens:\n",
    "            if token not in feature_vocab:\n",
    "                feature_vocab[token]=len(feature_vocab)\n",
    "            feats[feature_vocab[token]]=1\n",
    "        return feats\n",
    "\n",
    "    feature_vocab={}\n",
    "    data=[]\n",
    "    Y=[]\n",
    "\n",
    "    for caption in influencer1:\n",
    "        feats=featurize(caption, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(0)\n",
    "    for caption in influencer2:\n",
    "        feats=featurize(caption, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(1)\n",
    "    for caption in influencer3:\n",
    "        feats=featurize(caption, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(2)\n",
    "        \n",
    "    # shuffle the data\n",
    "    temp = list(zip(data, Y))\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "\n",
    "    # sparse representation\n",
    "    X=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "\n",
    "    for idx,feats in enumerate(data):\n",
    "        for f in feats:\n",
    "            X[idx,f]=feats[f]\n",
    "    \n",
    "    return X, Y, feature_vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "source": [
    "run_all(rachel_mansfield, minimalist_baker, delicioushealthyvideos, featurize_method_1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.567 NMI\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<162x2061 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 7398 stored elements in List of Lists format>,\n",
       " (2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0),\n",
       " array([2, 2, 0, 2, 2, 2, 2, 1, 0, 2, 1, 2, 2, 2, 0, 2, 1, 1, 2, 0, 2, 1,\n",
       "        1, 2, 2, 0, 1, 1, 2, 1, 0, 1, 2, 0, 2, 1, 1, 1, 2, 0, 2, 1, 1, 1,\n",
       "        1, 1, 0, 2, 0, 0, 1, 1, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 1, 1, 0, 1, 1, 1, 1, 2, 2,\n",
       "        1, 1, 1, 1, 2, 2, 0, 1, 0, 2, 2, 1, 0, 0, 2, 2, 1, 0, 0, 2, 2, 2,\n",
       "        2, 1, 2, 1, 2, 2, 2, 2, 0, 1, 1, 0, 0, 2, 1, 0, 2, 1, 2, 1, 2, 0,\n",
       "        2, 2, 2, 1, 2, 1, 2, 2, 2, 0, 0, 1, 1, 2, 1, 2, 1, 2, 0, 2, 2, 2,\n",
       "        0, 2, 2, 0, 0, 0, 2, 2], dtype=int32))"
      ]
     },
     "metadata": {},
     "execution_count": 228
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "source": [
    "#Bert helper fcn\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_average_across_text_tokens(string): \n",
    "    \n",
    "    # tokenize\n",
    "    inputs = tokenizer(string, return_tensors=\"pt\")\n",
    "    # convert input ids to words\n",
    "    tokens=tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "    if len(tokens)<=512:\n",
    "        outputs = model(**inputs)\n",
    "        bert_av = np.mean(outputs.last_hidden_state[0].detach().numpy(), axis=0)\n",
    "    else: \n",
    "        bert_av = np.zeros(768).tolist()\n",
    "        \n",
    "    return bert_av"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "source": [
    "# This function takes in a list of Instagram captions from two influencers, and returns:\n",
    "\n",
    "# X (matrix with average of BERT embeddings (768 features per caption))\n",
    "# Y (list of caption labels, with 1=minimalist_baker and 0=rachel_mansfield)\n",
    "    \n",
    "def featurize_method_3(influencer1, influencer2, influencer3):\n",
    "    \n",
    "    def featurize(caption, feature_vocab):\n",
    "                \n",
    "        feats=get_bert_average_across_text_tokens(caption)\n",
    "\n",
    "        return feats\n",
    "        \n",
    "    feature_vocab={}\n",
    "    data=[]\n",
    "    Y=[]\n",
    "\n",
    "    for caption in influencer1:\n",
    "        feats=featurize(caption, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(0)\n",
    "    for caption in influencer2:\n",
    "        feats=featurize(caption, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(1)\n",
    "    for caption in influencer3:\n",
    "        feats=featurize(caption, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(2)\n",
    "    temp = list(zip(data, Y))\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "\n",
    "    # not a sparse matrix\n",
    "    X=data\n",
    "        \n",
    "    return X, Y, feature_vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "source": [
    "X, Y, kmeans_labels = run_all(rachel_mansfield, minimalist_baker, delicioushealthyvideos, featurize_method_3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.580 NMI\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion (2 class kmeans)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I explored three methods for featurization to cluster Instagram captions. The first involved taking a sparse matrix across the entire vocabulary, where captions are rows and words are columns, with a binary word presence/absence. The second method simply took the total number of syllables and the total number of sentences in the captions, resulting in two features per caption (see visualization at the top of the notebook). The third involved taking the BERT embeddings of each token in the caption and averaging them, resulting in 768 features per caption. The BERT method performed best, with a NMI of ~0.9. Ultimately, style detection on Instagram captions is quite feasible, a task that scores significantly better than other types of style recognition (with similar methods for poem style segmentation scoring a NMI ~0.1, for example). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "source": [
    "X = pd.DataFrame(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "source": [
    "true_labels = pd.DataFrame(Y)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "source": [
    "kmeans_labels = pd.DataFrame(kmeans_labels)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "source": [
    "X['true_labels'] = true_labels\n",
    "X['kmeans_labels'] = kmeans_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "source": [
    "X.to_csv('PCA_insta_data1.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}